'use client';

import { useEffect, useMemo, useRef, useState } from 'react';
import { useParams, useRouter } from 'next/navigation';
import Link from 'next/link';
import { createClient, LiveTranscriptionEvents } from '@deepgram/sdk';

type Turn = { question: string; answer: string };

export default function InterviewSession() {
  const params = useParams();
  const router = useRouter();
  const interviewId = useMemo(() => (params.id as string) || '', [params]);

  const [currentQuestion, setCurrentQuestion] = useState<string>('');
  const [turns, setTurns] = useState<Turn[]>([]);
  const [loading, setLoading] = useState(false);
  const [playing, setPlaying] = useState(false);
  const [streaming, setStreaming] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [info, setInfo] = useState<string>('Listen to the question and answer.');
  const [liveText, setLiveText] = useState<string>('');
  const [aiCaption, setAiCaption] = useState<string>('');

  const deepgramKeyRef = useRef<string>('');
  const liveFinalRef = useRef<string>('');
  const accumulatedAnswerRef = useRef<string>(''); // Accumulate all final transcripts for full answer
  const lastFinalTranscriptRef = useRef<string>(''); // Track last final transcript to detect updates vs continuations
  const silenceTimerRef = useRef<NodeJS.Timeout | null>(null);
  const isStoppingRef = useRef<boolean>(false);
  const isStreamingRef = useRef<boolean>(false);
  const isStartingRef = useRef<boolean>(false); // Prevent multiple simultaneous starts

  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const micStreamRef = useRef<MediaStream | null>(null);
  const deepgramConnRef = useRef<any>(null);
  const audioRef = useRef<HTMLAudioElement | null>(null);
  const initializeRef = useRef<boolean>(false);

  useEffect(() => {
    if (!interviewId) return;
    
    // Prevent double initialization in React strict mode
    if (initializeRef.current) return;
    initializeRef.current = true;
    
    initialize();
    
    // Cleanup function
    return () => {
      initializeRef.current = false;
      isStoppingRef.current = true; // Prevent any new operations
      
      // Stop any playing audio
      if (audioRef.current) {
        audioRef.current.pause();
        audioRef.current = null;
      }
      
      // Clear timers
      if (silenceTimerRef.current) {
        clearTimeout(silenceTimerRef.current);
        silenceTimerRef.current = null;
      }
      
      // Directly clean up resources without calling stopLiveAnswer
      // to avoid state update issues during unmount
      if (mediaRecorderRef.current) {
        try {
          mediaRecorderRef.current.stop();
        } catch (e) {
          // Ignore errors during cleanup
        }
        mediaRecorderRef.current = null;
      }
      
      if (micStreamRef.current) {
        micStreamRef.current.getTracks().forEach((t) => t.stop());
        micStreamRef.current = null;
      }
      
      if (deepgramConnRef.current) {
        try {
          deepgramConnRef.current.finish();
        } catch (e) {
          // Ignore errors during cleanup
        }
        deepgramConnRef.current = null;
      }
      
      isStreamingRef.current = false;
      setStreaming(false);
    };
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [interviewId]);

  const initialize = async () => {
    setLoading(true);
    setError(null);
    
    // Check if this is a demo interview
    const isDemo = interviewId.startsWith('demo_');
    
    // For demo interviews, get data from sessionStorage
    let demoData = null;
    if (isDemo) {
      const stored = sessionStorage.getItem('demoInterview');
      if (stored) {
        const parsed = JSON.parse(stored);
        if (parsed.interviewId === interviewId) {
          demoData = {
            jobTitle: parsed.jobTitle,
            jobDescription: parsed.jobDescription,
            difficulty: parsed.difficulty,
          };
        }
      }
    }
    
    try {
      const response = await fetch('/api/interview/start', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ 
          interviewId,
          ...(isDemo && demoData ? { demoData } : {})
        }),
      });

      const data = await response.json();
      if (!response.ok || !data.success) {
        // For demo interviews, don't fail - just use the question
        if (isDemo && data.question) {
          setCurrentQuestion(data.question);
          setAiCaption(data.question);
          if (data.deepgramApiKey) {
            deepgramKeyRef.current = data.deepgramApiKey;
          }
          setInfo('Interview is starting...');
          await playSequence(['Hello, the interview is starting.', data.question], () => {
            console.log('üéµ All audio finished, starting microphone...');
            setTimeout(() => {
              if (!isStartingRef.current && !isStreamingRef.current && !streaming) {
                startLiveAnswer();
              } else {
                console.log('‚ö†Ô∏è Skipping startLiveAnswer - already active');
              }
            }, 100);
          });
          setLoading(false);
          return;
        }
        // If interview already in progress, fall back to fetching interview and proceed
        throw new Error(data.error || data.message || 'Interview start failed');
      }

      setCurrentQuestion(data.question);
      setAiCaption(data.question);
      if (data.deepgramApiKey) {
        deepgramKeyRef.current = data.deepgramApiKey;
      }
      setInfo('Interview is starting...');
      // We do not call startLiveAnswer immediately after playSequence
      // Microphone should open only after all audio finishes completely
      // startLiveAnswer is invoked in the playSequence onComplete callback
      await playSequence(['Hello, the interview is starting.', data.question], () => {
        console.log('üéµ All audio finished, starting microphone...');
        // Use setTimeout to ensure audio channel is fully released
        // Only start if not already starting/streaming
        setTimeout(() => {
          if (!isStartingRef.current && !isStreamingRef.current && !streaming) {
            startLiveAnswer();
          } else {
            console.log('‚ö†Ô∏è Skipping startLiveAnswer - already active');
          }
        }, 100);
      });
    } catch (err: any) {
      // For demo interviews, don't try to fetch from database
      if (isDemo) {
        setError('Demo interview initialization failed. Please try creating a new demo interview.');
        setLoading(false);
        return;
      }
      
      // Try to recover if already in progress (only for real interviews)
      const fallback = await fetch(`/api/interview/${interviewId}`);
      if (fallback.ok) {
        const data = await fallback.json();
        const q = data?.interview?.currentQuestion || '';
        setCurrentQuestion(q);
        setAiCaption(q);
        if (deepgramKeyRef.current) {
          // continue
        }
        setInfo('Interview is resuming...');
        if (q) {
          await playSequence(['Resuming the interview.', q], () => {
            console.log('üéµ Resume audio finished, starting microphone...');
            setTimeout(() => {
              // Force reset any stuck flags
              isStartingRef.current = false;
              isStoppingRef.current = false;
              if (!streaming && !isStreamingRef.current) {
                startLiveAnswer();
              } else {
                console.log('‚ö†Ô∏è Skipping startLiveAnswer - already active');
              }
            }, 300);
          });
        }
      } else {
      setError(err.message || 'Start error');
      }
    } finally {
      setLoading(false);
    }
  };

  /**
   * Play a single audio text and return a promise that resolves when audio finishes
   * This ensures proper sequential playback without overlap
   */
  const playQuestionAudio = async (text: string): Promise<void> => {
    // Stop any currently playing audio to prevent overlap
    if (audioRef.current) {
      audioRef.current.pause();
      audioRef.current.onended = null; // Remove old handlers
      audioRef.current.onerror = null;
      audioRef.current = null;
    }
    
    setPlaying(true);
    
    // Create and return a promise that resolves when audio finishes
    return new Promise<void>(async (resolve, reject) => {
      let resolved = false;
      const resolveOnce = () => {
        if (resolved) return;
        resolved = true;
        setPlaying(false);
        audioRef.current = null;
        resolve();
      };
      
      const rejectOnce = (error: Error) => {
        if (resolved) return;
        resolved = true;
        setPlaying(false);
        if (audioRef.current) {
          audioRef.current = null;
        }
        setError(error.message || 'Audio playback error');
        reject(error);
      };
      
      try {
        // Fetch audio URL
        const res = await fetch('/api/audio/question', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ text }),
        });
        
        const data = await res.json();
        if (!res.ok || !data.audioUrl) {
          throw new Error(data.error || 'Audio could not be generated');
        }
        
        // Create audio element
        const audio = new Audio(data.audioUrl);
        audioRef.current = audio;
        
        // Handle successful playback end
        audio.onended = () => {
          resolveOnce();
        };
        
        // Handle playback errors
        audio.onerror = (e) => {
          const error = new Error('Audio playback error');
          rejectOnce(error);
        };
        
        // Start playback
        try {
          await audio.play();
        } catch (playError: any) {
          // Playback failed (e.g., autoplay blocked)
          rejectOnce(new Error(playError.message || 'Audio could not play'));
        }
      } catch (err: any) {
        // Network or other error
        rejectOnce(err instanceof Error ? err : new Error(err.message || 'Audio could not be generated'));
      }
    });
  };

  /**
   * Play multiple audio texts in sequence, one after another
   * Only calls onComplete after ALL audio has finished + 500ms delay
   * Guarantees no overlap and single callback execution
   */
  const playSequence = async (texts: (string | number)[], onComplete?: () => void): Promise<void> => {
    // Filter out numbers (delays) and convert to strings
    const audioTexts = texts.filter((t): t is string => typeof t === 'string');
    
    if (audioTexts.length === 0) {
      // No audio to play, call onComplete immediately
      if (onComplete) {
        onComplete();
      }
      return;
    }
    
    let onCompleteCalled = false;
    const safeOnComplete = () => {
      if (onCompleteCalled) {
        console.log('‚ö†Ô∏è playSequence onComplete already called, ignoring duplicate');
        return;
      }
      onCompleteCalled = true;
      if (onComplete) {
        onComplete();
      }
    };
    
    try {
      // Play each audio sequentially - await ensures each finishes before next starts
      for (let i = 0; i < audioTexts.length; i++) {
        const text = audioTexts[i];
        
        console.log(`üéµ Playing audio ${i + 1}/${audioTexts.length}: ${text.substring(0, 50)}...`);
        
        // Wait for this audio to completely finish before moving to next
        await playQuestionAudio(text);
        
        // Small delay between audio clips for smooth transition (except after last)
        if (i < audioTexts.length - 1) {
          await new Promise(resolve => setTimeout(resolve, 200));
        }
      }
      
      console.log('üéµ All audio finished playing');
      
      // All audio finished - wait 500ms for Chrome audio channel to release
      // Then call onComplete callback (microphone will start after this)
      await new Promise(resolve => setTimeout(resolve, 500));
      
      // Call onComplete once, guaranteed
      safeOnComplete();
      
    } catch (error: any) {
      console.error('‚ùå Error in playSequence:', error);
      setError(error.message || 'Audio sequence error');
      
      // Even on error, call onComplete to allow recovery
      safeOnComplete();
    }
  };

  const startLiveAnswer = async () => {
    console.log('üé§ startLiveAnswer called');
    console.log('üé§ Current state:', {
      streaming,
      isStreamingRef: isStreamingRef.current,
      isStoppingRef: isStoppingRef.current,
      isStartingRef: isStartingRef.current,
      hasDeepgramKey: !!deepgramKeyRef.current,
      hasDeepgramConn: !!deepgramConnRef.current,
      hasMicStream: !!micStreamRef.current,
      hasMediaRecorder: !!mediaRecorderRef.current
    });
    
    // Prevent multiple simultaneous start attempts
    if (isStartingRef.current) {
      console.log('‚ö†Ô∏è Already starting, ignoring duplicate call');
      return;
    }
    
    // If already streaming, don't start again
    // Use ref as source of truth since state updates are async
    // Don't check streaming state - it may be stale
    if (isStreamingRef.current) {
      console.log('‚ö†Ô∏è Already streaming (ref check), cannot start');
      return;
    }
    
    // If state says streaming but ref says not, reset state
    if (streaming && !isStreamingRef.current) {
      console.log('‚ö†Ô∏è State says streaming but ref says not - resetting state');
      setStreaming(false);
    }
    
    // Clean up any leftover resources before starting
    if (deepgramConnRef.current) {
      console.log('üßπ Cleaning up leftover Deepgram connection before start');
      try {
        deepgramConnRef.current.finish();
      } catch (e) {
        console.warn('Error cleaning up Deepgram:', e);
      }
      deepgramConnRef.current = null;
    }
    
    if (micStreamRef.current) {
      console.log('üßπ Cleaning up leftover microphone stream before start');
      try {
        micStreamRef.current.getTracks().forEach(t => t.stop());
      } catch (e) {
        console.warn('Error cleaning up microphone:', e);
      }
      micStreamRef.current = null;
    }
    
    if (mediaRecorderRef.current) {
      console.log('üßπ Cleaning up leftover MediaRecorder before start');
      try {
        mediaRecorderRef.current.stop();
      } catch (e) {
        console.warn('Error cleaning up MediaRecorder:', e);
      }
      mediaRecorderRef.current = null;
    }
    
    // If stopping flag is true but we're not actually streaming, reset it
    if (isStoppingRef.current && !streaming && !isStreamingRef.current) {
      console.log('üîÑ Resetting stuck isStoppingRef flag');
      isStoppingRef.current = false;
    }
    
    // Set starting flag to prevent duplicate calls
    isStartingRef.current = true;
    
    setError(null);
    setLiveText('');
    liveFinalRef.current = '';
    accumulatedAnswerRef.current = ''; // Reset accumulated answer
    lastFinalTranscriptRef.current = ''; // Reset last final transcript tracker
    isStoppingRef.current = false; // Explicitly reset
    isStreamingRef.current = false; // Explicitly reset
    
    // Wait a moment to ensure cleanup is complete
    await new Promise(resolve => setTimeout(resolve, 100));

    if (!deepgramKeyRef.current) {
      const errorMsg = 'Deepgram key could not be retrieved.';
      console.error('‚ùå', errorMsg);
      setError(errorMsg);
      return;
    }

    try {
      console.log('üé§ Requesting microphone access...');
      const micStream = await navigator.mediaDevices.getUserMedia({ audio: true });
      console.log('‚úÖ Microphone access granted');
      micStreamRef.current = micStream;

      // Setup Deepgram live connection
      const dg = createClient(deepgramKeyRef.current);
      const connection = dg.listen.live({
        model: 'nova-2',
        language: 'en-US',
        smart_format: true,
        interim_results: true,
        punctuate: true,
        endpointing: 300, // Auto-endpoint after 300ms of silence
      });

      // Wait for connection to open - this is when we can start sending data
      let connectionOpenedRef = { value: false };
      
      connection.on(LiveTranscriptionEvents.Open, () => {
        console.log('‚úÖ Deepgram connection opened - ready to receive audio');
        connectionOpenedRef.value = true;
        setInfo('Microphone is active, you can speak...');
      });

      connection.on(LiveTranscriptionEvents.Transcript, (data: any) => {
        const transcript = data.channel?.alternatives?.[0]?.transcript || '';
        if (!transcript || transcript.trim().length === 0) return;
        
        const isFinal = data.is_final || false;
        const confidence = data.channel?.alternatives?.[0]?.confidence || 0;
        console.log('Transcript received:', { 
          transcript, 
          isFinal, 
          length: transcript.length,
          confidence 
        });
        
        if (isFinal) {
          // Final transcript - accumulate it for full answer
          const finalTranscript = transcript.trim();
          
          if (confidence > 0.5) { // Only use if confidence is reasonable
            // Deepgram sends final transcripts for complete segments
            // We need to accumulate them to get the full answer
            if (accumulatedAnswerRef.current) {
              const lastFinal = lastFinalTranscriptRef.current.toLowerCase().trim();
              const newFinal = finalTranscript.toLowerCase().trim();
              
              // Check if this is an update to the last segment or a new segment
              if (lastFinal && newFinal.includes(lastFinal) && newFinal.length > lastFinal.length) {
                // This is an updated/expanded version of the last segment
                // Only replace if the last segment is at the end of accumulated answer
                const accumulatedLower = accumulatedAnswerRef.current.toLowerCase();
                const lastIndex = accumulatedLower.lastIndexOf(lastFinal);
                const isAtEnd = lastIndex !== -1 && (lastIndex + lastFinal.length >= accumulatedLower.length - 1);
                
                if (isAtEnd) {
                  // Replace the last segment with the new expanded version
                  const before = accumulatedAnswerRef.current.substring(0, lastIndex).trim();
                  accumulatedAnswerRef.current = before 
                    ? (before + ' ' + finalTranscript).trim()
                    : finalTranscript;
                } else {
                  // Last segment not at end - this is likely a new segment, append it
                  accumulatedAnswerRef.current = accumulatedAnswerRef.current + ' ' + finalTranscript;
                }
                lastFinalTranscriptRef.current = finalTranscript;
              } else if (lastFinal && lastFinal.includes(newFinal) && lastFinal.length > newFinal.length) {
                // Duplicate or shorter version - ignore
                console.log('‚ö†Ô∏è Ignoring duplicate/shorter final transcript:', finalTranscript);
              } else if (newFinal !== lastFinal) {
                // This is a new segment - append it
                accumulatedAnswerRef.current = accumulatedAnswerRef.current + ' ' + finalTranscript;
                lastFinalTranscriptRef.current = finalTranscript;
              } else {
                // Same as last - ignore duplicate
                console.log('‚ö†Ô∏è Ignoring duplicate final transcript:', finalTranscript);
              }
            } else {
              // First final transcript
              accumulatedAnswerRef.current = finalTranscript;
              lastFinalTranscriptRef.current = finalTranscript;
            }
            
            // Also update liveFinalRef for backward compatibility
            liveFinalRef.current = accumulatedAnswerRef.current;
            
            // For subtitle display, show the latest final transcript (real-time feedback)
            // This gives users immediate visual feedback without breaking the flow
            setLiveText(finalTranscript);
            console.log('‚úÖ Final transcript received:', finalTranscript);
            console.log('üìù Accumulated full answer:', accumulatedAnswerRef.current);
          } else if (!accumulatedAnswerRef.current) {
            // Use even low confidence if we have nothing else
            accumulatedAnswerRef.current = finalTranscript;
            lastFinalTranscriptRef.current = finalTranscript;
            liveFinalRef.current = finalTranscript;
            setLiveText(finalTranscript);
          }
          
          // Always start timer for auto-submit when we have any final transcript
          if (finalTranscript.length > 3) {
            console.log('‚è±Ô∏è Starting silence timer for final transcript...');
            resetSilenceTimer();
          }
        } else {
          // Interim transcript - show live for real-time subtitle feedback
          const interimTranscript = transcript.trim();
          setLiveText(interimTranscript);
          
          // Save interim as fallback if we don't have any accumulated answer yet
          if (!accumulatedAnswerRef.current && interimTranscript.length > 5) {
            accumulatedAnswerRef.current = interimTranscript;
            liveFinalRef.current = interimTranscript;
          }
          console.log('üìù Interim transcript:', interimTranscript);
          
          // Start timer for interim transcripts too - if we have meaningful text
          if (interimTranscript.length > 5) {
            resetSilenceTimer();
          }
        }
      });

      connection.on(LiveTranscriptionEvents.Error, (err: any) => {
        console.error('‚ùå Deepgram error:', err);
        setError('Live STT error: ' + (err?.message || 'Unknown error'));
        // Try to restart if error occurs
        setTimeout(() => {
          if (!streaming && !isStreamingRef.current && !isStoppingRef.current) {
            startLiveAnswer();
          }
        }, 2000);
      });

      connection.on(LiveTranscriptionEvents.Close, (event: any) => {
        console.log('üîå Deepgram connection closed', event);
        // This can happen if:
        // 1. We called finish() intentionally (normal)
        // 2. Connection timed out due to no data
        // 3. Network error
        // 4. Server closed connection
        
        // If we're still supposed to be streaming, this is unexpected
        if (isStreamingRef.current || streaming) {
          console.warn('‚ö†Ô∏è Deepgram closed unexpectedly while streaming!');
          // Reset state so we can restart if needed
          isStreamingRef.current = false;
          setStreaming(false);
          isStoppingRef.current = false;
          isStartingRef.current = false;
        }
      });

      deepgramConnRef.current = connection;

      // Wait for connection to open (Open event fires)
      // We'll wait max 2 seconds for connection to open
      let openTimeout: NodeJS.Timeout;
      await new Promise<void>((resolve) => {
        if (connectionOpenedRef.value) {
          resolve();
          return;
        }
        
        // Check if connection opened
        const checkOpened = setInterval(() => {
          if (connectionOpenedRef.value) {
            clearInterval(checkOpened);
            if (openTimeout) clearTimeout(openTimeout);
            resolve();
          }
        }, 50);
        
        // Timeout after 2 seconds - start anyway
        openTimeout = setTimeout(() => {
          clearInterval(checkOpened);
          console.warn('‚ö†Ô∏è Deepgram Open event not received in 2s, starting anyway...');
          resolve();
        }, 2000);
      });

      console.log('üì° Deepgram ready, starting MediaRecorder...');

      // MediaRecorder MIME type - cross-browser compatibility
      // Chrome/Edge: audio/webm;codecs=opus ‚úì
      // Safari: doesn't support webm, will use default
      const mimeType = MediaRecorder.isTypeSupported("audio/webm;codecs=opus")
        ? "audio/webm;codecs=opus"
        : MediaRecorder.isTypeSupported("audio/webm")
        ? "audio/webm"
        : undefined; // Browser will use default
      
      const recorder = new MediaRecorder(micStream, {
        mimeType: mimeType,
      });
      mediaRecorderRef.current = recorder;
      
      console.log('üìπ MediaRecorder created with mimeType:', mimeType || 'default');

      recorder.ondataavailable = async (e) => {
        if (!e.data || e.data.size === 0) {
          console.log('‚ö†Ô∏è MediaRecorder: Empty data chunk received');
          return;
        }
        
        try {
          const buf = await e.data.arrayBuffer();
          if (deepgramConnRef.current && connection.getReadyState() === 1) {
            connection.send(buf);
            console.log(`üì§ Sent ${buf.byteLength} bytes to Deepgram`);
          } else {
            console.warn('‚ö†Ô∏è Cannot send data - connection not open');
          }
        } catch (sendError) {
          console.error('‚ùå Error sending data to Deepgram:', sendError);
        }
      };

      // Start recorder immediately - it will send data every 300ms
      // Deepgram needs continuous data stream to keep connection alive
      recorder.start(300);
      console.log('üéôÔ∏è MediaRecorder started, sending data every 300ms');
      
      isStreamingRef.current = true;
      setStreaming(true);
      isStartingRef.current = false; // Clear starting flag - we're now streaming
      setInfo('Microphone is active, you can speak...');
      
      console.log('‚úÖ Microphone recording started successfully - waiting for speech...');
    } catch (err: any) {
      console.error('‚ùå Error starting microphone:', err);
      isStartingRef.current = false; // Clear starting flag on error
      setError('Live recording could not start: ' + (err.message || ''));
      stopLiveAnswer(false);
    }
  };

  const stopLiveAnswer = async (send: boolean = true) => {
    // Prevent multiple calls - check both state and ref
    if (isStoppingRef.current || (!streaming && !isStreamingRef.current)) {
      if (isStoppingRef.current) {
        console.log('‚ö†Ô∏è Already stopping, ignoring duplicate stopLiveAnswer call');
      } else {
        console.log('‚ö†Ô∏è Already stopped, ignoring stopLiveAnswer call');
      }
      return;
    }
    
    isStoppingRef.current = true;
    console.log('üõë Stopping live answer, send:', send);
    
    // Clear silence timer first
    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
      silenceTimerRef.current = null;
    }
    
    // Get the best transcript we have BEFORE stopping
    // Use accumulated answer (full answer) if available, otherwise fall back to liveFinalRef or liveText
    const transcriptToSend = accumulatedAnswerRef.current || liveFinalRef.current || liveText || '';
    const trimmedTranscript = transcriptToSend.trim();
    console.log('üì§ Transcript to send:', trimmedTranscript, 'send:', send, 'length:', trimmedTranscript.length);
    console.log('üì§ Accumulated answer:', accumulatedAnswerRef.current);
    console.log('üì§ Live final ref:', liveFinalRef.current);
    console.log('üì§ Live text:', liveText);
    
    // Store in const for use in async closure
    const finalTranscriptToSend = trimmedTranscript;
    
    // Stop recording first (this will stop sending data to Deepgram)
    if (mediaRecorderRef.current) {
      try {
        mediaRecorderRef.current.stop();
        console.log('üõë MediaRecorder stopped');
      } catch (e) {
        console.warn('Error stopping MediaRecorder:', e);
      }
      mediaRecorderRef.current = null;
    }
    
    // Wait a bit for any remaining data to be sent, then close Deepgram connection
    await new Promise(resolve => setTimeout(resolve, 300));
    
    // Close Deepgram connection properly
    if (deepgramConnRef.current) {
      try {
        const readyState = deepgramConnRef.current.getReadyState();
        console.log('üîå Deepgram connection state before close:', readyState);
        
        if (readyState === 1) { // OPEN
          deepgramConnRef.current.finish();
          console.log('‚úÖ Deepgram connection finished');
        } else {
          console.log('‚ö†Ô∏è Deepgram connection already closed or closing');
        }
      } catch (e) {
        console.warn('Error finishing Deepgram connection:', e);
      }
      deepgramConnRef.current = null;
    }
    
    // Stop microphone tracks completely
    if (micStreamRef.current) {
      try {
        micStreamRef.current.getTracks().forEach((track) => {
          track.stop();
          console.log('üõë Microphone track stopped:', track.kind);
        });
        micStreamRef.current = null;
        console.log('‚úÖ Microphone stream released');
      } catch (e) {
        console.warn('Error stopping microphone tracks:', e);
      }
    }
    
    // Wait a bit more to ensure all resources are released
    await new Promise(resolve => setTimeout(resolve, 200));
    
    // Reset all state flags - do refs first (synchronous)
    isStreamingRef.current = false;
    isStoppingRef.current = false;
    isStartingRef.current = false; // Reset starting flag when stopped
    
    // Then update state (asynchronous, but refs are already updated)
    setStreaming(false);
    
    console.log('‚úÖ All resources cleaned up, state reset');
    
    // Process transcript after everything is stopped
    if (send && finalTranscriptToSend.length > 0) {
      console.log('‚úÖ Sending answer to server:', finalTranscriptToSend);
      setInfo('Analyzing your answer...');
      sendAnswer(finalTranscriptToSend);
    } else if (send) {
      console.warn('‚ö†Ô∏è No transcript to send, restarting...');
      setInfo('No answer captured, listening again...');
      // Restart listening if no answer was captured
      setTimeout(() => {
        if (!streaming && !isStreamingRef.current && !isStoppingRef.current) {
          startLiveAnswer();
        }
      }, 1000);
    }
  };

  const resetSilenceTimer = () => {
    // Clear existing timer
    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
      silenceTimerRef.current = null;
    }
    
    // Don't start timer if already stopping or not streaming
    if (isStoppingRef.current || !streaming || !isStreamingRef.current) {
      console.log('‚è±Ô∏è Cannot start silence timer - not streaming or stopping');
      return;
    }
    
    console.log('‚è±Ô∏è Starting 3-second silence timer...');
    
    // Start new timer
    silenceTimerRef.current = setTimeout(() => {
      console.log('‚è±Ô∏è Silence timer fired - checking for transcript...');
      
      // Double-check we're still streaming and not stopping
      if (isStoppingRef.current || !streaming || !isStreamingRef.current) {
        console.log('‚è±Ô∏è Silence timer cancelled - no longer streaming');
        return;
      }
      
      // Get the best transcript we have (use accumulated answer for full answer)
      const transcriptToSubmit = accumulatedAnswerRef.current || liveFinalRef.current || liveText || '';
      const trimmedTranscript = transcriptToSubmit.trim();
      
      if (trimmedTranscript.length > 0) {
        console.log('‚è±Ô∏è Silence detected (3s) - submitting transcript:', trimmedTranscript);
        console.log('üì§ Transcript length:', trimmedTranscript.length);
        console.log('üì§ Accumulated answer:', accumulatedAnswerRef.current);
        stopLiveAnswer(true);
      } else {
        console.log('‚è±Ô∏è Silence detected but no transcript found, continuing to listen...');
        // Restart timer to keep listening
        resetSilenceTimer();
      }
    }, 3000); // Submit after 3s of silence after speech
  };

  const sendAnswer = async (answer: string) => {
    if (!currentQuestion || !answer || answer.trim().length === 0) {
      console.log('Cannot send answer - missing question or answer');
      setInfo('No answer captured, listening again...');
      setTimeout(() => {
        if (!streaming && !isStreamingRef.current && !isStoppingRef.current) {
          startLiveAnswer();
        }
      }, 1000);
      return;
    }
    
    // Reset all flags before sending answer to ensure clean state for next question
    console.log('üîÑ Resetting flags before sending answer');
    isStartingRef.current = false;
    isStoppingRef.current = false;
    isStreamingRef.current = false;
    // Also reset state to ensure it's in sync
    setStreaming(false);
    
    console.log('Sending answer:', answer, 'for question:', currentQuestion);
    setLoading(true);
    setError(null);
    setLiveText(''); // Clear live text
    liveFinalRef.current = ''; // Clear final ref
    accumulatedAnswerRef.current = ''; // Clear accumulated answer
    lastFinalTranscriptRef.current = ''; // Clear last final transcript tracker
    
    // For demo interviews, get job title from sessionStorage
    let demoJobTitle = null;
    if (interviewId.startsWith('demo_')) {
      const stored = sessionStorage.getItem('demoInterview');
      if (stored) {
        const parsed = JSON.parse(stored);
        if (parsed.interviewId === interviewId) {
          demoJobTitle = parsed.jobTitle;
        }
      }
    }
    
    try {
      const res = await fetch('/api/interview/respond', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          interviewId,
          question: currentQuestion,
          answer: answer.trim(),
          ...(demoJobTitle ? { demoJobTitle } : {}),
        }),
      });
      
      const data = await res.json();
      console.log('Response from server:', data);
      
      if (!res.ok || !data.success) {
        throw new Error(data.error || data.message || 'Response could not be processed');
      }

      if (!data.nextQuestion) {
        throw new Error('Next question could not be fetched');
      }

      // Save the turn
      const newTurn = { question: currentQuestion, answer: answer.trim() };
      setTurns((prev) => [...prev, newTurn]);
      
      // Update interview transcript in database (for real interviews only)
      if (!interviewId.startsWith('demo_')) {
        try {
          // Update transcript in background (don't wait for it)
          fetch('/api/interview/update-transcript', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
              interviewId,
              question: currentQuestion,
              answer: answer.trim(),
            }),
          }).catch(err => console.error('Error updating transcript:', err));
        } catch (err) {
          console.error('Error updating transcript:', err);
        }
      }
      
      // Update to next question
      setCurrentQuestion(data.nextQuestion);
      setAiCaption(data.nextQuestion);
      setInfo('Playing the next question...');
      
      // Play thank you message and next question
      // Microphone should open only after all audio finishes
      console.log('üéµ Starting to play next question sequence...');
      console.log('üéµ Next question:', data.nextQuestion);
      
      await playSequence([
        'Thank you for your answer. Here is the next question.',
        data.nextQuestion
      ], () => {
        console.log('üéµ Question audio finished, starting microphone...');
        // Restart listening for next answer - only after all audio finishes
        // Wait longer to ensure all state is cleared and resources are released
        setTimeout(() => {
          console.log('üé§ Checking state before starting microphone...', {
            isStartingRef: isStartingRef.current,
            isStreamingRef: isStreamingRef.current,
            streaming,
            isStoppingRef: isStoppingRef.current,
            hasDeepgramConn: !!deepgramConnRef.current,
            hasMicStream: !!micStreamRef.current,
            hasMediaRecorder: !!mediaRecorderRef.current
          });
          
          // Force cleanup any remaining resources
          if (deepgramConnRef.current) {
            console.log('üîÑ Cleaning up leftover Deepgram connection');
            try {
              deepgramConnRef.current.finish();
            } catch (e) {
              console.warn('Error cleaning up Deepgram:', e);
            }
            deepgramConnRef.current = null;
          }
          
          if (micStreamRef.current) {
            console.log('üîÑ Cleaning up leftover microphone stream');
            try {
              micStreamRef.current.getTracks().forEach(t => t.stop());
            } catch (e) {
              console.warn('Error cleaning up microphone:', e);
            }
            micStreamRef.current = null;
          }
          
          if (mediaRecorderRef.current) {
            console.log('üîÑ Cleaning up leftover MediaRecorder');
            try {
              mediaRecorderRef.current.stop();
            } catch (e) {
              console.warn('Error cleaning up MediaRecorder:', e);
            }
            mediaRecorderRef.current = null;
          }
          
          // Force reset ALL flags and state - use refs as source of truth
          isStartingRef.current = false;
          isStreamingRef.current = false;
          isStoppingRef.current = false;
          
          // Force state update - use functional update to ensure it happens
          setStreaming((prev) => {
            if (prev) {
              console.log('üîÑ Resetting streaming state from', prev, 'to false');
            }
            return false;
          });
          
          // Wait a bit more to ensure cleanup is complete and state is updated
          setTimeout(() => {
            // Use refs as source of truth, not state (state updates are async)
            // Double-check and force reset if needed
            if (isStreamingRef.current) {
              console.log('üîÑ Force resetting isStreamingRef in timeout');
              isStreamingRef.current = false;
            }
            if (isStoppingRef.current) {
              console.log('üîÑ Force resetting isStoppingRef in timeout');
              isStoppingRef.current = false;
            }
            if (isStartingRef.current) {
              console.log('üîÑ Force resetting isStartingRef in timeout');
              isStartingRef.current = false;
            }
            
            // Now check again with fresh refs
            if (!isStreamingRef.current && !isStoppingRef.current && !isStartingRef.current) {
              console.log('üé§ Starting microphone for next question...');
              startLiveAnswer();
            } else {
              console.log('‚ö†Ô∏è Skipping startLiveAnswer - refs indicate still active:', {
                isStreamingRef: isStreamingRef.current,
                isStoppingRef: isStoppingRef.current,
                isStartingRef: isStartingRef.current
              });
            }
          }, 300);
        }, 500); // Increased delay to ensure state is cleared
      });
      
      console.log('‚úÖ sendAnswer completed successfully');
    } catch (err: any) {
      console.error('‚ùå Error in sendAnswer:', err);
      console.error('‚ùå Error stack:', err.stack);
      setError(err.message || 'Answer submission error');
      setInfo('An error occurred, listening again...');
      
      // Even if there's an error, try to restart listening
      // This ensures the interview can continue even if one question fails
      setTimeout(() => {
        // Use refs as source of truth
        if (!isStreamingRef.current && !isStoppingRef.current && !isStartingRef.current) {
          console.log('üîÑ Attempting to restart after error...');
          startLiveAnswer();
        }
      }, 2000);
    } finally {
      setLoading(false);
    }
  };

  return (
    <div className="min-h-screen bg-gradient-to-br from-cyan-400 to-white dark:from-slate-900 dark:to-slate-950 py-10 px-4">
      <div className="max-w-5xl mx-auto space-y-6">
        <div className="flex items-center justify-between">
          <Link
            href={`/interview/${interviewId}`}
            className="text-sm text-cyan-100 hover:text-white underline-offset-2 hover:underline"
          >
            ‚Üê Interview Details
          </Link>
          <span className="text-sm text-gray-100">{info}</span>
        </div>

        <div className="grid md:grid-cols-2 gap-4">
          {/* User column */}
          <div className="bg-white/95 dark:bg-slate-900/90 backdrop-blur-sm rounded-3xl shadow-lg p-6 space-y-4 border border-white/40 dark:border-slate-700">
            <div className="flex items-center gap-3">
              <div className="h-10 w-10 rounded-full bg-gradient-to-br from-cyan-500 to-cyan-600 text-white font-semibold flex items-center justify-center">
                U
              </div>
              <span className="px-3 py-1 text-xs font-semibold rounded-full bg-green-100 text-green-700">
                User
              </span>
              {streaming && <span className="text-xs text-gray-500">Listening...</span>}
            </div>

            <div className="min-h-[60px] rounded-2xl bg-gray-50 dark:bg-slate-800 border border-gray-200 dark:border-slate-700 p-4">
              <p className="text-xs text-gray-500 dark:text-slate-300 mb-1">Captions (live)</p>
              <p className="text-gray-800 dark:text-slate-100">
                {liveText || 'Start speaking...'}
              </p>
            </div>
            
            {/* Manual submit button */}
            {streaming && liveText && liveText.trim().length > 0 && (
              <button
                onClick={() => {
                  console.log('üì§ Manual submit clicked');
                  // Use accumulated answer for full answer, fallback to liveFinalRef or liveText
                  const transcriptToSend = accumulatedAnswerRef.current || liveFinalRef.current || liveText;
                  if (transcriptToSend && transcriptToSend.trim().length > 0) {
                    stopLiveAnswer(true);
                  }
                }}
                className="w-full px-4 py-2 bg-green-600 text-white rounded-xl hover:bg-green-500 transition-colors font-medium"
              >
                Submit Answer
              </button>
            )}
          </div>

          {/* AI column */}
          <div className="bg-white/95 dark:bg-slate-900/90 backdrop-blur-sm rounded-3xl shadow-lg p-6 space-y-4 border border-white/40 dark:border-slate-700">
            <div className="flex items-center gap-3">
              <div className="h-10 w-10 rounded-full bg-gradient-to-br from-purple-500 to-indigo-700 text-white font-semibold flex items-center justify-center">
                AI
              </div>
              <span className="px-3 py-1 text-xs font-semibold rounded-full bg-purple-100 text-purple-700">
                AI
              </span>
              {playing && <span className="text-xs text-gray-500">Playing audio...</span>}
            </div>

            <div className="flex gap-3 flex-wrap">
              {/* Replay button was removed; flow is automatic */}
            </div>

            <div className="min-h-[60px] rounded-2xl bg-gray-50 dark:bg-slate-800 border border-gray-200 dark:border-slate-700 p-4">
              <p className="text-xs text-gray-500 dark:text-slate-300 mb-1">Captions (AI)</p>
              <p className="text-gray-800 dark:text-slate-100">
                {aiCaption || currentQuestion || 'Loading question...'}
              </p>
            </div>
          </div>
        </div>

        <div className="bg-white/95 dark:bg-slate-900/90 rounded-3xl shadow-lg p-6">
          <h3 className="text-lg font-semibold text-gray-800 dark:text-slate-100 mb-4">
            Conversation History
          </h3>
            {turns.length === 0 ? (
              <p className="text-sm text-gray-500 dark:text-slate-400">No answers yet.</p>
          ) : (
            <div className="space-y-4">
              {turns.map((turn, idx) => (
                <div key={idx} className="border border-gray-200 dark:border-slate-700 rounded-2xl p-4">
                  <p className="text-sm text-gray-500 dark:text-slate-400 mb-2">
                    Soru {idx + 1}
                  </p>
                  <p className="text-gray-800 dark:text-slate-100 font-medium mb-2">
                    {turn.question}
                  </p>
                  <p className="text-gray-700 dark:text-slate-200">
                    <span className="text-gray-500 dark:text-slate-400 font-semibold mr-2">
                      Answer:
                    </span>
                    {turn.answer}
                  </p>
                </div>
              ))}
            </div>
          )}
        </div>

        {error && (
          <div className="bg-red-50 dark:bg-red-900/40 border border-red-200 dark:border-red-500/60 text-red-700 dark:text-red-200 px-4 py-3 rounded-2xl">
            {error}
          </div>
        )}

        <div className="flex justify-end gap-4">
          <button
            onClick={async () => {
              // Check if this is a demo interview
              const isDemo = interviewId.startsWith('demo_');
              
              if (isDemo) {
                // For demo interviews, generate feedback and redirect to demo feedback page
                try {
                  setLoading(true);
                  setInfo('Finishing interview and generating feedback...');
                  
                  // Stop any active recording
                  if (streaming || isStreamingRef.current) {
                    stopLiveAnswer(false);
                  }

                  // Get demo data from sessionStorage
                  const demoData = sessionStorage.getItem('demoInterview');
                  let jobTitle = 'Demo Interview';
                  let jobDescription = '';
                  let difficulty = 'medium';
                  
                  if (demoData) {
                    const parsed = JSON.parse(demoData);
                    if (parsed.interviewId === interviewId) {
                      jobTitle = parsed.jobTitle || jobTitle;
                      jobDescription = parsed.jobDescription || jobDescription;
                      difficulty = parsed.difficulty || difficulty;
                    }
                  }

                  // Collect all turns into transcript format
                  const transcript = turns.map(t => `Q: ${t.question}\nA: ${t.answer}`).join('\n\n');
                  const questions = turns.map(t => t.question);
                  
                  // Complete interview and generate feedback
                  const response = await fetch('/api/interview/complete', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ 
                      interviewId,
                      transcript,
                      questions,
                      jobTitle,
                      jobDescription,
                      difficulty,
                    }),
                  });

                  const data = await response.json();
                  
                  if (data.success && data.feedback) {
                    // Store feedback in sessionStorage for the demo feedback page
                    sessionStorage.setItem('demoFeedback', JSON.stringify({
                      feedback: data.feedback,
                      jobTitle,
                      jobDescription,
                      difficulty,
                    }));
                    
                    // Redirect to demo feedback page
                    router.push('/demo/feedback');
                  } else {
                    // If feedback generation failed, still redirect but show message
                    console.warn('Demo feedback generation failed:', data.error || data.message);
                    router.push('/demo/feedback');
                  }
                } catch (error) {
                  console.error('Error completing demo interview:', error);
                  // Still redirect to feedback page even on error
                  router.push('/demo/feedback');
                } finally {
                  setLoading(false);
                }
                return;
              }

              // For real interviews, complete and generate feedback
              try {
                setLoading(true);
                setInfo('Finishing interview and generating feedback...');
                
                // Stop any active recording
                if (streaming || isStreamingRef.current) {
                  stopLiveAnswer(false);
                }

                // Collect all turns into transcript format
                const transcript = turns.map(t => `Q: ${t.question}\nA: ${t.answer}`).join('\n\n');
                
                // Complete interview and generate feedback
                const response = await fetch('/api/interview/complete', {
                  method: 'POST',
                  headers: { 'Content-Type': 'application/json' },
                  body: JSON.stringify({ interviewId }),
                });

                const data = await response.json();
                
                if (data.success) {
                  // Redirect to interview detail page to show feedback
                  router.push(`/interview/${interviewId}`);
                } else {
                  alert('Error while completing the interview: ' + data.error);
                  router.push(`/interview/${interviewId}`);
                }
              } catch (error) {
                console.error('Error completing interview:', error);
                alert('An error occurred while completing the interview');
                router.push(`/interview/${interviewId}`);
              } finally {
                setLoading(false);
              }
            }}
            disabled={loading}
            className="px-6 py-2 bg-gradient-to-r from-cyan-500 to-cyan-600 text-white rounded-xl hover:shadow-lg transition-all duration-300 font-medium disabled:opacity-50 disabled:cursor-not-allowed"
          >
            {loading ? 'Finishing...' : 'Finish Interview and Get Feedback'}
          </button>
        </div>
      </div>
    </div>
  );
}


